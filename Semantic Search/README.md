# Semantic Search with Embedding Models
## Overview
This project focuses on conducting a semantic search using various sentence transformer models and evaluating their performance. The primary goal is to compare the models based on time elapsed, total error, model size, and the quality of results.

## Methodology:

This project revolves around the exploration of semantic search methodologies employing model embeddings. In the initial phase, extensive research was conducted to gain insights on semantic search utilizing embedding models. Subsequently, after surveying notable models on sbert.net, I opted for the top seven models celebrated for their efficacy in semantic search.

## Experimental Design:

The experiment was structured using a dataset comprising 21 questions and corresponding answers from "To Kill a Mockingbird." This dataset underwent careful curation to ensure its suitability for robust model evaluation. Fine-tuning adjustments were made to optimize the experimental setup.

## Result Evaluation:

The evaluation process included an in-depth analysis of the top 10 results generated by each model for every question. A metric was devised to quantify the proximity of the model's answers to the actual answers, providing a quantitative measure of the model's overall performance.

### Significance of Top Three Results:

Beyond the top 10 results, a specific focus was placed on evaluating the top three outcomes from each model. This analysis aimed to understand the frequency with which the correct answer appeared among the initial suggestions, offering additional insights into model efficacy.

## Project Structure

```plaintext
Semantic Search/
|-- semantic-search.py
|-- data/
|   |-- questions_and_answers.py
|   |-- doc_file.py
|-- utils/
|   |-- check_sentence_similarity.py
|   |-- plot_model_stats.py 
```

`semantic-search.py`: This is the main script responsible for implementing the semantic search using sentence transformer models.

`data/questions_and_answers.py`: This folder contains predefined questions and answers used for evaluation in the semantic search script.

`data/doc_file.py`: This file contains the text that will be used for searching in the semantic search system.

`utils/check_sentence_similarity.py`: The utils folder contains a module with functions to check the similarity between sentences.

### Usage
To run the semantic search script, use the following command:
```bash
python semantic-search.py
```

### Dependencies
Ensure you have the necessary dependencies installed using the following command:
```bash
# to load text embeding models
pip install sentence-transformers

# to compare similarity of two sentences
pip install fuzzywuzzy

# to convert plain doc text into chunks of sentences
pip install nltk

```

## Model Statistics
The following models, renowned for their outstanding performance in semantic search, have been carefully selected based on recommendations from the Sentence-BERT (SBERT) library. You can find a detailed list of pretrained models in the [SBERT documentation](https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models):

![SBERT Models](https://imgur.com/a/ynbe3HA){:width="300px"}

### 1. Model Size
Model size is an important consideration, especially for applications with limited storage capacity. The all-MiniLM-L12-v2 model has a relatively larger size, while the msmarco-distilbert-dot-v5 model is more lightweight.

### 2. Time Elapsed Comparison

![SBERT Models](https://imgur.com/a/9qqpt4m){:width="300px"}

The bar chart illustrates the time elapsed for each sentence transformer model during the semantic search. Notably, the multi-qa-MiniLM-L6-cos-v1 model exhibited the fastest performance, making it a suitable choice for real-time applications. On the other hand, the all-mpnet-base-v2 model, while accurate, had a longer processing time.

### 3. Total Error Comparison

![SBERT Models](https://imgur.com/a/2on9swf){:width="300px"}

The bar chart compares the total error for each model, providing insights into their accuracy. The multi-qa-mpnet-base-dot-v1 model achieved the lowest total error, indicating high precision in retrieving relevant results. However, the all-distilroberta-v1 model showed a higher error rate, suggesting a trade-off between speed and accuracy.

### 4. Results Comparison

<!-- ![SBERT Models](https://imgur.com/a/ufgNvbj){:width="300px"} -->
<img width="200" height="450" src="https://imgur.com/a/ufgNvbj">

The stacked bar chart displays the number of first, second, and third results found by each model, offering a glimpse into the quality of results. Model multi-qa-mpnet-base-dot-v1 consistently performed well in finding the first result, demonstrating its effectiveness in delivering relevant information promptly. 

## Analysis

Based on the model statistics:
```markdown
    - Model Size (MB) Comparison:
        * Model size is an essential consideration, with the all-MiniLM-L12-v2 model having a larger size, and the msmarco-distilbert-dot-v5 model being more lightweight.

    - Time Elapsed:
        * Model multi-qa-MiniLM-L6-cos-v1 demonstrated the fastest performance, making it suitable for real-time applications.
        Model all-mpnet-base-v2 had the longest time elapsed, providing high accuracy at the cost of processing time.

    - Total Error:
        * Model multi-qa-mpnet-base-dot-v1 achieved the lowest total error, emphasizing its precision in retrieving accurate results.
        Model all-distilroberta-v1 showed a higher total error, suggesting a trade-off between speed and accuracy.

    - Results Comparison:
        * Model multi-qa-mpnet-base-dot-v1 consistently performed well in finding the first result, showcasing its effectiveness in delivering relevant information promptly.

```

## Conclusion

The analysis provides valuable insights into the performance of different sentence transformer models for semantic search. Considerations such as time elapsed, total error, model size, and result quality should be taken into account when selecting the most suitable model for specific use cases. The trade-off between speed and accuracy is evident, and the choice of the model depends on the application requirements.








